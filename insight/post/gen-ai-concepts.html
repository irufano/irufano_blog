<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta property="og:type" content="website"/><meta property="og:image" content="https://irufano.github.io/images/insight-default.svg"/><meta property="og:image:alt" content="Irufano Insight"/><meta property="og:image:type" content="image/svg"/><meta property="og:image:width" content="900"/><meta property="og:image:height" content="800"/><meta property="og:locale" content="en_US"/><title>Generative AI Key Concepts</title><meta name="robots" content="index,follow"/><meta name="description" content="Machine learning is a subset of AI that involves developing algorithms that allow computers to learn from &amp; make predictions or decisions based on data."/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@site"/><meta name="twitter:creator" content="@handle"/><meta property="og:title" content="Generative AI Key Concepts"/><meta property="og:description" content="Machine learning is a subset of AI that involves developing algorithms that allow computers to learn from &amp; make predictions or decisions based on data."/><meta property="og:url" content="https://irufano.github.io/insight/post/gen-ai-concepts"/><meta property="og:image" content="https://irufano.github.io/images/insight-default.svg"/><meta property="og:image:alt" content="Irufano"/><meta property="og:image:type" content="image/svg"/><meta property="og:image:width" content="800"/><meta property="og:image:height" content="600"/><meta property="og:site_name" content="Irufano Insight"/><link rel="canonical" href="https://irufano.github.io/insight/post/gen-ai-concepts"/><meta name="next-head-count" content="25"/><link rel="preload" href="https://irufano.github.io/_next/static/css/6ad7948154ab6bce.css" as="style"/><link rel="stylesheet" href="https://irufano.github.io/_next/static/css/6ad7948154ab6bce.css" data-n-g=""/><link rel="preload" href="https://irufano.github.io/_next/static/css/addb6b2ca46a17db.css" as="style"/><link rel="stylesheet" href="https://irufano.github.io/_next/static/css/addb6b2ca46a17db.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="https://irufano.github.io/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="https://irufano.github.io/_next/static/chunks/webpack-aba38411314208c8.js" defer=""></script><script src="https://irufano.github.io/_next/static/chunks/framework-ecc4130bc7a58a64.js" defer=""></script><script src="https://irufano.github.io/_next/static/chunks/main-335786cffa1638a5.js" defer=""></script><script src="https://irufano.github.io/_next/static/chunks/pages/_app-bf4b1c3fa6e9777e.js" defer=""></script><script src="https://irufano.github.io/_next/static/chunks/155-4e7468b7051ba820.js" defer=""></script><script src="https://irufano.github.io/_next/static/chunks/874-1fafd686af5fe50a.js" defer=""></script><script src="https://irufano.github.io/_next/static/chunks/pages/insight/post/%5Bslug%5D-5d0dea37da74a22d.js" defer=""></script><script src="https://irufano.github.io/_next/static/DLEiHtd6RpNZKc-Yv-HAa/_buildManifest.js" defer=""></script><script src="https://irufano.github.io/_next/static/DLEiHtd6RpNZKc-Yv-HAa/_ssgManifest.js" defer=""></script></head><body><div id="__next"> <div class="__variable_e85e31 font-ubuntu flex flex-col min-h-screen"><nav class="fixed w-full bg-surface dark:bg-surface-dark text-white z-50 transition-transform duration-30 shadow-sm translate-y-0"><div class="container mx-auto p-4 flex justify-between items-center"><div class="md:hidden flex items-center"><button class="text-text dark:text-white focus:outline-none"><svg xmlns="http://www.w3.org/2000/svg" viewBox="180.276 2174.957 627.448 490.086" width="627.448pt" height="490.086pt" class="w-6 h-6"><path d=" M 219.276 2213.957 L 768.724 2213.957 M 219.276 2420 L 768.724 2420 M 219.276 2626.043 L 348 2626.043 L 504 2626.043 L 528 2626.043" fill="none" stroke-width="56" stroke="currentColor" stroke-linejoin="round" stroke-linecap="round"></path></svg></button></div><div class="text-xl font-bold"><a href="/insight"><svg xmlns="http://www.w3.org/2000/svg" viewBox="654 914.275 1705.89 187.45" width="1705.89pt" height="187.45pt" fill="currentColor" class="h-5 w-auto md:h-6 text-text dark:text-white"><path d=" M 1686.259 1065.544 L 1669.891 1065.544 L 1669.891 947.354 L 1686.259 947.354 L 1686.259 1065.544 L 1686.259 1065.544 Z  M 1731.743 1065.544 L 1715.892 1065.544 L 1715.892 947.354 L 1731.743 947.354 L 1789.115 1025.229 L 1789.115 1025.229 Q 1789.976 1026.263 1791.355 1028.33 L 1791.355 1028.33 L 1791.355 1028.33 Q 1792.733 1030.398 1794.111 1032.379 L 1794.111 1032.379 L 1794.111 1032.379 Q 1795.49 1034.36 1796.351 1035.911 L 1796.351 1035.911 L 1797.04 1035.911 L 1797.04 1035.911 Q 1797.04 1033.154 1797.04 1030.484 L 1797.04 1030.484 L 1797.04 1030.484 Q 1797.04 1027.813 1797.04 1025.229 L 1797.04 1025.229 L 1797.04 947.354 L 1813.063 947.354 L 1813.063 1065.544 L 1798.074 1065.544 L 1740.013 986.292 L 1740.013 986.292 Q 1738.807 984.396 1736.481 980.951 L 1736.481 980.951 L 1736.481 980.951 Q 1734.155 977.505 1732.604 975.437 L 1732.604 975.437 L 1731.743 975.437 L 1731.743 975.437 Q 1731.743 978.194 1731.743 980.865 L 1731.743 980.865 L 1731.743 980.865 Q 1731.743 983.535 1731.743 986.292 L 1731.743 986.292 L 1731.743 1065.544 L 1731.743 1065.544 Z  M 1886.113 1067.612 L 1886.113 1067.612 L 1886.113 1067.612 Q 1876.465 1067.612 1867.592 1065.717 L 1867.592 1065.717 L 1867.592 1065.717 Q 1858.719 1063.821 1851.828 1059.6 L 1851.828 1059.6 L 1851.828 1059.6 Q 1844.936 1055.379 1840.888 1048.66 L 1840.888 1048.66 L 1840.888 1048.66 Q 1836.839 1041.941 1836.839 1032.12 L 1836.839 1032.12 L 1836.839 1032.12 Q 1836.839 1031.259 1836.925 1030.398 L 1836.925 1030.398 L 1836.925 1030.398 Q 1837.011 1029.536 1837.011 1028.675 L 1837.011 1028.675 L 1853.551 1028.675 L 1853.551 1028.675 Q 1853.551 1029.191 1853.465 1030.311 L 1853.465 1030.311 L 1853.465 1030.311 Q 1853.378 1031.431 1853.378 1032.293 L 1853.378 1032.293 L 1853.378 1032.293 Q 1853.378 1039.184 1857.513 1044.008 L 1857.513 1044.008 L 1857.513 1044.008 Q 1861.648 1048.832 1868.971 1051.158 L 1868.971 1051.158 L 1868.971 1051.158 Q 1876.293 1053.484 1885.596 1053.484 L 1885.596 1053.484 L 1885.596 1053.484 Q 1889.731 1053.484 1894.297 1052.967 L 1894.297 1052.967 L 1894.297 1052.967 Q 1898.863 1052.45 1903.084 1051.158 L 1903.084 1051.158 L 1903.084 1051.158 Q 1907.305 1049.866 1910.664 1047.712 L 1910.664 1047.712 L 1910.664 1047.712 Q 1914.024 1045.559 1916.005 1042.372 L 1916.005 1042.372 L 1916.005 1042.372 Q 1917.987 1039.184 1917.987 1034.532 L 1917.987 1034.532 L 1917.987 1034.532 Q 1917.987 1028.675 1914.627 1024.798 L 1914.627 1024.798 L 1914.627 1024.798 Q 1911.267 1020.922 1905.754 1018.423 L 1905.754 1018.423 L 1905.754 1018.423 Q 1900.241 1015.925 1893.263 1013.944 L 1893.263 1013.944 L 1893.263 1013.944 Q 1886.286 1011.963 1878.877 1010.067 L 1878.877 1010.067 L 1878.877 1010.067 Q 1871.469 1008.172 1864.491 1005.674 L 1864.491 1005.674 L 1864.491 1005.674 Q 1857.513 1003.176 1852 999.472 L 1852 999.472 L 1852 999.472 Q 1846.487 995.768 1843.127 990.254 L 1843.127 990.254 L 1843.127 990.254 Q 1839.768 984.741 1839.768 976.816 L 1839.768 976.816 L 1839.768 976.816 Q 1839.768 969.407 1842.783 963.55 L 1842.783 963.55 L 1842.783 963.55 Q 1845.798 957.692 1851.742 953.643 L 1851.742 953.643 L 1851.742 953.643 Q 1857.686 949.594 1866.472 947.441 L 1866.472 947.441 L 1866.472 947.441 Q 1875.259 945.287 1886.63 945.287 L 1886.63 945.287 L 1886.63 945.287 Q 1895.934 945.287 1903.945 947.268 L 1903.945 947.268 L 1903.945 947.268 Q 1911.957 949.25 1918.073 953.298 L 1918.073 953.298 L 1918.073 953.298 Q 1924.189 957.347 1927.635 963.55 L 1927.635 963.55 L 1927.635 963.55 Q 1931.081 969.752 1931.081 978.539 L 1931.081 978.539 L 1931.081 980.606 L 1914.885 980.606 L 1914.885 978.022 L 1914.885 978.022 Q 1914.885 972.164 1911.267 968.029 L 1911.267 968.029 L 1911.267 968.029 Q 1907.649 963.894 1901.361 961.654 L 1901.361 961.654 L 1901.361 961.654 Q 1895.072 959.415 1887.147 959.415 L 1887.147 959.415 L 1887.147 959.415 Q 1876.81 959.415 1869.918 961.396 L 1869.918 961.396 L 1869.918 961.396 Q 1863.027 963.377 1859.667 967.081 L 1859.667 967.081 L 1859.667 967.081 Q 1856.307 970.786 1856.307 975.437 L 1856.307 975.437 L 1856.307 975.437 Q 1856.307 980.778 1859.667 984.396 L 1859.667 984.396 L 1859.667 984.396 Q 1863.027 988.015 1868.54 990.34 L 1868.54 990.34 L 1868.54 990.34 Q 1874.053 992.666 1881.117 994.561 L 1881.117 994.561 L 1881.117 994.561 Q 1888.181 996.457 1895.503 998.438 L 1895.503 998.438 L 1895.503 998.438 Q 1902.825 1000.419 1909.803 1002.917 L 1909.803 1002.917 L 1909.803 1002.917 Q 1916.781 1005.416 1922.38 1009.12 L 1922.38 1009.12 L 1922.38 1009.12 Q 1927.979 1012.824 1931.253 1018.423 L 1931.253 1018.423 L 1931.253 1018.423 Q 1934.526 1024.023 1934.526 1032.12 L 1934.526 1032.12 L 1934.526 1032.12 Q 1934.526 1044.697 1928.324 1052.537 L 1928.324 1052.537 L 1928.324 1052.537 Q 1922.122 1060.376 1911.267 1063.994 L 1911.267 1063.994 L 1911.267 1063.994 Q 1900.413 1067.612 1886.113 1067.612 Z  M 1975.014 1065.544 L 1958.647 1065.544 L 1958.647 947.354 L 1975.014 947.354 L 1975.014 1065.544 L 1975.014 1065.544 Z  M 2057.885 1067.612 L 2057.885 1067.612 L 2057.885 1067.612 Q 2029.285 1067.612 2014.382 1052.623 L 2014.382 1052.623 L 2014.382 1052.623 Q 1999.479 1037.634 1999.479 1006.449 L 1999.479 1006.449 L 1999.479 1006.449 Q 1999.479 986.119 2006.371 972.509 L 2006.371 972.509 L 2006.371 972.509 Q 2013.262 958.898 2026.701 952.092 L 2026.701 952.092 L 2026.701 952.092 Q 2040.139 945.287 2059.436 945.287 L 2059.436 945.287 L 2059.436 945.287 Q 2071.496 945.287 2081.316 947.871 L 2081.316 947.871 L 2081.316 947.871 Q 2091.137 950.456 2098.287 955.71 L 2098.287 955.71 L 2098.287 955.71 Q 2105.437 960.965 2109.313 968.891 L 2109.313 968.891 L 2109.313 968.891 Q 2113.19 976.816 2113.19 987.67 L 2113.19 987.67 L 2096.478 987.67 L 2096.478 987.67 Q 2096.478 980.089 2093.721 974.662 L 2093.721 974.662 L 2093.721 974.662 Q 2090.964 969.235 2085.882 965.789 L 2085.882 965.789 L 2085.882 965.789 Q 2080.799 962.344 2074.08 960.879 L 2074.08 960.879 L 2074.08 960.879 Q 2067.361 959.415 2059.78 959.415 L 2059.78 959.415 L 2059.78 959.415 Q 2049.443 959.415 2041.345 961.999 L 2041.345 961.999 L 2041.345 961.999 Q 2033.248 964.583 2027.734 970.097 L 2027.734 970.097 L 2027.734 970.097 Q 2022.221 975.61 2019.292 984.224 L 2019.292 984.224 L 2019.292 984.224 Q 2016.363 992.839 2016.363 1004.727 L 2016.363 1004.727 L 2016.363 1008.345 L 2016.363 1008.345 Q 2016.363 1024.367 2021.274 1034.36 L 2021.274 1034.36 L 2021.274 1034.36 Q 2026.184 1044.353 2035.487 1048.919 L 2035.487 1048.919 L 2035.487 1048.919 Q 2044.791 1053.484 2057.885 1053.484 L 2057.885 1053.484 L 2057.885 1053.484 Q 2071.151 1053.484 2079.766 1049.78 L 2079.766 1049.78 L 2079.766 1049.78 Q 2088.38 1046.076 2092.601 1038.495 L 2092.601 1038.495 L 2092.601 1038.495 Q 2096.822 1030.914 2096.822 1019.371 L 2096.822 1019.371 L 2096.822 1017.304 L 2055.128 1017.304 L 2055.128 1003.348 L 2113.362 1003.348 L 2113.362 1065.544 L 2100.957 1065.544 L 2099.234 1050.555 L 2099.234 1050.555 Q 2094.582 1056.585 2088.208 1060.376 L 2088.208 1060.376 L 2088.208 1060.376 Q 2081.833 1064.166 2074.166 1065.889 L 2074.166 1065.889 L 2074.166 1065.889 Q 2066.499 1067.612 2057.885 1067.612 Z  M 2158.157 1065.544 L 2141.789 1065.544 L 2141.789 947.354 L 2158.157 947.354 L 2158.157 998.007 L 2222.42 998.007 L 2222.42 947.354 L 2238.96 947.354 L 2238.96 1065.544 L 2222.42 1065.544 L 2222.42 1012.307 L 2158.157 1012.307 L 2158.157 1065.544 L 2158.157 1065.544 Z  M 2313.906 1065.544 L 2297.538 1065.544 L 2297.538 961.654 L 2257.05 961.654 L 2257.05 947.354 L 2354.738 947.354 L 2354.738 961.654 L 2313.906 961.654 L 2313.906 1065.544 L 2313.906 1065.544 Z " fill="currentColor" class="text-text dark:text-gray-300"></path><path d=" M 867.788 1065.544 L 836.949 1065.544 L 836.949 947.182 L 867.788 947.182 L 867.788 1065.544 L 867.788 1065.544 Z  M 924.471 1065.544 L 893.632 1065.544 L 893.632 947.182 L 962.375 947.182 L 962.375 947.182 Q 975.469 947.182 984.255 952.092 L 984.255 952.092 L 984.255 952.092 Q 993.042 957.003 997.435 965.531 L 997.435 965.531 L 997.435 965.531 Q 1001.829 974.059 1001.829 984.569 L 1001.829 984.569 L 1001.829 984.569 Q 1001.829 995.94 996.574 1004.813 L 996.574 1004.813 L 996.574 1004.813 Q 991.319 1013.686 981.843 1018.682 L 981.843 1018.682 L 1005.619 1065.544 L 971.161 1065.544 L 951.52 1023.851 L 924.471 1023.851 L 924.471 1065.544 L 924.471 1065.544 Z  M 924.471 970.613 L 924.471 1000.936 L 956.345 1000.936 L 956.345 1000.936 Q 962.719 1000.936 966.596 996.715 L 966.596 996.715 L 966.596 996.715 Q 970.472 992.494 970.472 985.43 L 970.472 985.43 L 970.472 985.43 Q 970.472 980.778 968.749 977.505 L 968.749 977.505 L 968.749 977.505 Q 967.026 974.231 963.925 972.422 L 963.925 972.422 L 963.925 972.422 Q 960.824 970.613 956.345 970.613 L 956.345 970.613 L 924.471 970.613 L 924.471 970.613 Z  M 1076.085 1067.612 L 1076.085 1067.612 L 1076.085 1067.612 Q 1059.028 1067.612 1046.71 1062.271 L 1046.71 1062.271 L 1046.71 1062.271 Q 1034.391 1056.93 1027.844 1046.248 L 1027.844 1046.248 L 1027.844 1046.248 Q 1021.297 1035.566 1021.297 1019.716 L 1021.297 1019.716 L 1021.297 947.182 L 1052.309 947.182 L 1052.309 1019.026 L 1052.309 1019.026 Q 1052.309 1030.398 1058.339 1036.944 L 1058.339 1036.944 L 1058.339 1036.944 Q 1064.369 1043.491 1076.085 1043.491 L 1076.085 1043.491 L 1076.085 1043.491 Q 1087.801 1043.491 1094.003 1036.944 L 1094.003 1036.944 L 1094.003 1036.944 Q 1100.205 1030.398 1100.205 1019.026 L 1100.205 1019.026 L 1100.205 947.182 L 1131.045 947.182 L 1131.045 1019.716 L 1131.045 1019.716 Q 1131.045 1035.566 1124.498 1046.248 L 1124.498 1046.248 L 1124.498 1046.248 Q 1117.951 1056.93 1105.719 1062.271 L 1105.719 1062.271 L 1105.719 1062.271 Q 1093.486 1067.612 1076.085 1067.612 Z  M 1187.383 1065.544 L 1156.544 1065.544 L 1156.544 947.182 L 1247.34 947.182 L 1247.34 971.303 L 1187.383 971.303 L 1187.383 996.801 L 1240.104 996.801 L 1240.104 1020.405 L 1187.383 1020.405 L 1187.383 1065.544 L 1187.383 1065.544 Z  M 1277.663 1065.544 L 1245.961 1065.544 L 1290.412 947.182 L 1327.454 947.182 L 1371.904 1065.544 L 1338.997 1065.544 L 1331.933 1045.042 L 1284.726 1045.042 L 1277.663 1065.544 L 1277.663 1065.544 Z  M 1300.749 996.974 L 1292.135 1021.955 L 1324.353 1021.955 L 1315.911 996.974 L 1315.911 996.974 Q 1315.221 995.078 1314.274 992.236 L 1314.274 992.236 L 1314.274 992.236 Q 1313.326 989.393 1312.379 986.206 L 1312.379 986.206 L 1312.379 986.206 Q 1311.431 983.018 1310.57 979.745 L 1310.57 979.745 L 1310.57 979.745 Q 1309.708 976.471 1308.847 974.059 L 1308.847 974.059 L 1307.641 974.059 L 1307.641 974.059 Q 1306.952 977.333 1305.659 981.468 L 1305.659 981.468 L 1305.659 981.468 Q 1304.367 985.602 1303.075 989.737 L 1303.075 989.737 L 1303.075 989.737 Q 1301.783 993.872 1300.749 996.974 L 1300.749 996.974 L 1300.749 996.974 Z  M 1414.632 1065.544 L 1386.032 1065.544 L 1386.032 947.182 L 1413.426 947.182 L 1456.498 1002.142 L 1456.498 1002.142 Q 1457.704 1003.52 1459.685 1006.105 L 1459.685 1006.105 L 1459.685 1006.105 Q 1461.667 1008.689 1463.562 1011.36 L 1463.562 1011.36 L 1463.562 1011.36 Q 1465.457 1014.03 1466.318 1015.581 L 1466.318 1015.581 L 1467.18 1015.408 L 1467.18 1015.408 Q 1467.18 1011.618 1467.18 1008.086 L 1467.18 1008.086 L 1467.18 1008.086 Q 1467.18 1004.554 1467.18 1002.142 L 1467.18 1002.142 L 1467.18 947.182 L 1495.78 947.182 L 1495.78 1065.544 L 1468.558 1065.544 L 1422.902 1007.655 L 1422.902 1007.655 Q 1420.662 1004.899 1418.681 1001.97 L 1418.681 1001.97 L 1418.681 1001.97 Q 1416.699 999.041 1415.493 997.146 L 1415.493 997.146 L 1414.632 997.318 L 1414.632 997.318 Q 1414.632 1000.075 1414.632 1002.917 L 1414.632 1002.917 L 1414.632 1002.917 Q 1414.632 1005.76 1414.632 1007.655 L 1414.632 1007.655 L 1414.632 1065.544 L 1414.632 1065.544 Z  M 1578.478 1067.612 L 1578.478 1067.612 L 1578.478 1067.612 Q 1559.01 1067.612 1545.14 1060.806 L 1545.14 1060.806 L 1545.14 1060.806 Q 1531.271 1054.001 1523.863 1040.39 L 1523.863 1040.39 L 1523.863 1040.39 Q 1516.454 1026.779 1516.454 1006.277 L 1516.454 1006.277 L 1516.454 1006.277 Q 1516.454 985.775 1523.863 972.25 L 1523.863 972.25 L 1523.863 972.25 Q 1531.271 958.725 1545.14 951.92 L 1545.14 951.92 L 1545.14 951.92 Q 1559.01 945.115 1578.478 945.115 L 1578.478 945.115 L 1578.478 945.115 Q 1597.947 945.115 1611.816 951.92 L 1611.816 951.92 L 1611.816 951.92 Q 1625.685 958.725 1633.094 972.25 L 1633.094 972.25 L 1633.094 972.25 Q 1640.502 985.775 1640.502 1006.277 L 1640.502 1006.277 L 1640.502 1006.277 Q 1640.502 1026.779 1633.094 1040.39 L 1633.094 1040.39 L 1633.094 1040.39 Q 1625.685 1054.001 1611.816 1060.806 L 1611.816 1060.806 L 1611.816 1060.806 Q 1597.947 1067.612 1578.478 1067.612 Z  M 1578.478 1043.491 L 1578.478 1043.491 L 1578.478 1043.491 Q 1585.887 1043.491 1591.572 1041.252 L 1591.572 1041.252 L 1591.572 1041.252 Q 1597.258 1039.012 1601.134 1034.705 L 1601.134 1034.705 L 1601.134 1034.705 Q 1605.011 1030.398 1606.906 1024.195 L 1606.906 1024.195 L 1606.906 1024.195 Q 1608.801 1017.993 1608.801 1010.24 L 1608.801 1010.24 L 1608.801 1002.659 L 1608.801 1002.659 Q 1608.801 994.734 1606.906 988.531 L 1606.906 988.531 L 1606.906 988.531 Q 1605.011 982.329 1601.134 978.022 L 1601.134 978.022 L 1601.134 978.022 Q 1597.258 973.715 1591.572 971.475 L 1591.572 971.475 L 1591.572 971.475 Q 1585.887 969.235 1578.478 969.235 L 1578.478 969.235 L 1578.478 969.235 Q 1570.898 969.235 1565.212 971.475 L 1565.212 971.475 L 1565.212 971.475 Q 1559.527 973.715 1555.736 978.022 L 1555.736 978.022 L 1555.736 978.022 Q 1551.946 982.329 1550.051 988.531 L 1550.051 988.531 L 1550.051 988.531 Q 1548.155 994.734 1548.155 1002.659 L 1548.155 1002.659 L 1548.155 1010.24 L 1548.155 1010.24 Q 1548.155 1017.993 1550.051 1024.195 L 1550.051 1024.195 L 1550.051 1024.195 Q 1551.946 1030.398 1555.736 1034.705 L 1555.736 1034.705 L 1555.736 1034.705 Q 1559.527 1039.012 1565.212 1041.252 L 1565.212 1041.252 L 1565.212 1041.252 Q 1570.898 1043.491 1578.478 1043.491 Z " fill="currentColor"></path><path d="M 696.047 1059.572 L 660.118 1023.643 C 652.037 1015.562 651.95 1002.528 659.924 994.554 L 659.924 994.554 C 667.898 986.58 680.932 986.667 689.013 994.748 L 724.942 1030.677 C 733.023 1038.758 733.11 1051.792 725.136 1059.766 L 725.136 1059.766 C 717.162 1067.74 704.128 1067.653 696.047 1059.572 Z" fill="#0CD79A"></path><path d="M 696.258 1031.027 L 732.192 995.098 C 740.274 987.017 753.31 986.93 761.285 994.904 L 761.285 994.904 C 769.26 1002.878 769.173 1015.912 761.092 1023.993 L 725.158 1059.922 C 717.076 1068.003 704.04 1068.09 696.065 1060.116 L 696.065 1060.116 C 688.09 1052.142 688.177 1039.108 696.258 1031.027 Z" fill="#46ECBA"></path><circle cx="710.6039726458765" cy="968.7405337030478" r="18.780480559001603" fill="rgb(12,215,154)"></circle></svg></a></div><div class="hidden md:flex space-x-4 items-center text-md font-medium"><a href="/"><h3 class="text-text mr-2 dark:text-text-dark hover:text-primary dark:hover:text-primary-dark">Home</h3></a><a href="/tools"><h3 class="text-text mr-2 dark:text-text-dark hover:text-primary dark:hover:text-primary-dark">Tools</h3></a><a href="/insight"><h3 class="text-text mr-2 dark:text-text-dark hover:text-primary dark:hover:text-primary-dark">Insight</h3></a><button class="flex items-center justify-center bg-gray-100 dark:bg-gray-800 p-2 rounded-full focus:outline-none 
        
        " aria-label="Toggle Theme"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search w-auto h-5 text-secondary"><g><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></g></svg></button><button class="flex items-center justify-center bg-gray-100 dark:bg-gray-800 p-2 rounded-full focus:outline-none" aria-label="Toggle Theme"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon w-auto h-5 text-primary"><g><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></g></svg></button></div><div class="md:hidden flex items-center"><button class="flex items-center justify-center bg-gray-100 dark:bg-gray-800 p-2 rounded-full focus:outline-none" aria-label="Toggle Theme"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon w-auto h-5 text-primary"><g><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></g></svg></button></div></div></nav><main class="font-ubuntu flex-grow"><div class="container mx-auto pt-20 md:pt-24 lg:flex lg:flex-row"><div class="mx-auto p-4"><article class="prose prose-lg dark:prose-dark mx-auto"><h1 class="text-4xl font-bold mb-4 text-text dark:text-text-heading">Generative AI Key Concepts</h1><div class="flex space-x-4 mb-8"><p class="flex items-center mt-2 text-xs md:text-sm text-gray-700 dark:text-gray-300"><span><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar mr-2 text-primary"><g><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></g></svg></span>2024-09-24</p><p class="flex items-center mt-2 text-xs md:text-sm text-gray-700 dark:text-gray-300"><span><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock mr-2 text-primary"><g><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></g></svg></span>7 min read</p><p class="flex items-center mt-2 text-xs md:text-sm"><span><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-user mr-2 text-primary"><g><path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></g></svg></span><a class="text-gray-700 dark:text-gray-300 no-underline font-normal hover:text-primary dark:hover:text-primary-dark" href="https://github.com/irufano">irufano</a></p></div><div class="block lg:hidden mb-8 lg:mb-0"><div class="shadow-sm rounded-md border-2 border-gray-200 dark:border-gray-700"><div class="flex justify-between items-center px-4 py-2 cursor-pointer"><div class="text-base font-medium text-gray-500 dark:text-gray-400">Contents</div><svg class="w-5 h-5 transform transition-transform duration-300 " fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path></svg></div></div></div><div class="mb-4"><img alt="Generative AI Key Concepts image" loading="lazy" decoding="async" data-nimg="fill" class="!relative rounded-md shadow-sm mb-0" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/images/AI-diagram.png"/><div class="flex justify-center mt-4 text-xs"><p class="p-0 mt-0 "><i> <!-- -->Thumbnail<!-- --> <a class="no-underline" rel="stylesheet" href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/images/AI-diagram.png">Credit</a></i></p></div></div><div class="prose prose-lg dark:prose-dark"><h2 id="definition">Definition</h2>
<p>Generative AI refers to a type of artificial intelligence that creates new content, such as images, text, audio, or even video, based on patterns and data it has learned. What makes it a fantastic technology is that it democratizes AI, anyone can use it with as little as a text prompt, a sentence written in a natural language. There's no need for you to learn a language like Java or SQL to accomplish something worthwhile, all you need is to use your language, state what you want and out comes a suggestion from an AI model. The applications and impact for this is huge, you write or understand reports, write applications and much more, all in seconds.</p>
<p>After decades of research in the AI field, a new model architecture – called Transformer – overcame the limits of RNNs, being able to get much longer sequences of text as input. Transformers are based on the attention mechanism, enabling the model to give different weights to the inputs it receives, ‘paying more attention’ where the most relevant information is concentrated, regardless of their order in the text sequence.</p>
<p>Most of the recent generative AI models – also known as Large Language Models (LLMs), since they work with textual inputs and outputs – are indeed based on this architecture. What’s interesting about these models – trained on a huge amount of unlabeled data from diverse sources like books, articles and websites – is that they can be adapted to a wide variety of tasks and generate grammatically correct text with a semblance of creativity. So, not only did they incredibly enhance the capacity of a machine to ‘understand’ an input text, but they enabled their capacity to generate an original response in human language.</p>
<h2 id="how-do-large-language-models-work">How do large language models work?</h2>
<h3 id="tokenization">Tokenization</h3>
<p>Large Language Models receive a text as input and generate a text as output. However, being statistical models, they work much better with numbers than text sequences. That’s why every input to the model is processed by a tokenizer, before being used by the core model. A token is a chunk of text – consisting of a variable number of characters, so the tokenizer's main task is splitting the input into an array of tokens. Then, each token is mapped with a token index, which is the integer encoding of the original text chunk.</p>
<p><img src="https://github.com/irufano/generative-ai-for-beginners/raw/main/01-introduction-to-genai/images/tokenizer-example.png?WT.mc_id=academic-105485-koreyst" alt="image"></p>
<h3 id="predicting-output-tokens">Predicting output tokens</h3>
<p>Given n tokens as input (with max n varying from one model to another), the model is able to predict one token as output. This token is then incorporated into the input of the next iteration, in an expanding window pattern, enabling a better user experience of getting one (or multiple) sentence as an answer. This explains why, if you ever played with ChatGPT, you might have noticed that sometimes it looks like it stops in the middle of a sentence.</p>
<h3 id="selection-process">Selection process</h3>
<p>The output token is chosen by the model according to its probability of occurring after the current text sequence. This is because the model predicts a probability distribution over all possible ‘next tokens’, calculated based on its training. However, not always the token with the highest probability is chosen from the resulting distribution. A degree of randomness is added to this choice, in a way that the model acts in a non-deterministic fashion - we do not get the exact same output for the same input. This degree of randomness is added to simulate the process of creative thinking and it can be tuned using a model parameter called temperature.</p>
<h2 id="custom-knowledge-base">Custom Knowledge Base</h2>
<p>To add a knowledge base to an OpenAI-powered chatbot or application, you can use a variety of approaches depending on how dynamic and structured you want your knowledge base to be. Here are some common methods:</p>
<h3 id="embedding-knowledge-with-fine-tuning">Embedding Knowledge with Fine-Tuning</h3>
<ul>
<li><span class="strong-block">Fine-tuning</span>: You can fine-tune the OpenAI model on your specific knowledge base or domain. This involves training the model on a custom dataset (such as product information, FAQs, internal documents) to improve its responses.
<ul>
<li>Steps:
<ul>
<li>Collect data (e.g., text documents, structured data).</li>
<li>Format it into the proper input-output pairs (prompts and completions).</li>
<li>Fine-tune the model using OpenAI's fine-tuning API.</li>
</ul>
</li>
</ul>
</li>
<li><span class="strong-block">Use case</span>: Best for cases where knowledge is mostly static and doesn’t change often.</li>
</ul>
<h3 id="use-knowledge-base-api-and-dynamic-retrieval">Use Knowledge Base API and Dynamic Retrieval</h3>
<ul>
<li><span class="strong-block">External APIs</span>: Integrate the chatbot with a real-time knowledge base, such as a MySQL database, content management system, or external API.
<ul>
<li><span class="strong-block">Method</span>: Implement a retrieval mechanism that allows the chatbot to fetch answers from your dynamic database based on user queries. For instance, using vector embeddings to match user queries with the most relevant content in the database.</li>
<li><span class="strong-block">Tools</span>: You can use libraries like langchain to facilitate this process, creating a pipeline that pulls data dynamically based on user input.</li>
</ul>
</li>
<li><span class="strong-block">Use case</span>: Useful when your knowledge base is large, changes frequently, or needs to handle complex queries.</li>
</ul>
<h3 id="contextual-knowledge-with-prompts">Contextual Knowledge with Prompts</h3>
<ul>
<li><span class="strong-block">Long prompts with context</span>: You can inject specific information about your knowledge base directly into the prompt as context.
<ul>
<li><span class="strong-block">Method</span>: Before each query, construct a detailed prompt that includes relevant information from your knowledge base, which the model will use to provide context-aware answers.</li>
<li><span class="strong-block">Use case</span>: Best for handling small sets of information or specific queries where the information needed is known beforehand.</li>
</ul>
</li>
</ul>
<h3 id="augmenting-with-vector-databases-eg-pinecone-faiss">Augmenting with Vector Databases (e.g., Pinecone, FAISS)</h3>
<ul>
<li><span class="strong-block">Vector search</span>: Store your knowledge base as embeddings in a vector database (like Pinecone, Weaviate, or FAISS). When a user asks a question, convert it to a vector, search for the most relevant knowledge entry, and use the result as part of the answer.
<ul>
<li><span class="strong-block">Method</span>: Convert text to embeddings using OpenAI or other models, and retrieve the most similar data points in response to a user query.</li>
<li><span class="strong-block">Use case:</span> Perfect for large datasets where similarity search is required to find relevant answers.</li>
</ul>
</li>
</ul>
<h3 id="hybrid-approach-with-langchain-and-fastapi">Hybrid Approach with LangChain and FastAPI</h3>
<ul>
<li>LangChain: LangChain enables building large language model (LLM) applications that can dynamically interact with databases and external systems. It helps set up a chatbot that can fetch data from knowledge bases like databases, APIs, and even web pages.
<ul>
<li>Steps:
<ol>
<li>Store Knowledge: Define a knowledge base, which could be a SQL database, document store, or API.</li>
<li>Query Execution: Use an LLM with the LangChain framework to query the knowledge base dynamically.</li>
<li>FastAPI Integration: Combine this with FastAPI for serving the chatbot with dynamic knowledge retrieval.</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="document-retrieval-with-plugins-or-middleware">Document Retrieval with Plugins or Middleware</h3>
<ul>
<li>OpenAI Plugins: If you're using OpenAI in an application, consider building or using a plugin or middleware that acts as a bridge to your knowledge base. For example, retrieving documents, querying databases, or even triggering workflows (e.g., customer support tickets) based on the query.</li>
<li>Use case: Ideal for more complex enterprise solutions, where OpenAI can function alongside existing systems.</li>
</ul>
<h3 id="hybrid-qa-pipelines-llm--traditional-search">Hybrid QA Pipelines (LLM + Traditional Search)</h3>
<ul>
<li><span class="strong-block">Combining LLMs with traditional search</span>: You can combine the capabilities of language models with traditional search mechanisms like Elasticsearch or other keyword-based systems.
<ul>
<li><span class="strong-block">Method</span>: First, use search engines to retrieve relevant documents, and then pass those documents to OpenAI for processing and summarization.</li>
<li><span class="strong-block">Use case</span>: Works well in applications where quick retrieval from a large corpus of documents is needed, and then those documents are summarized or clarified by the LLM.</li>
</ul>
</li>
</ul>
</div><div class="my-8"><h4 class="text-lg font-semibold text-text dark:text-text-dark">Tags:</h4><div class="list-none flex flex-wrap gap-2 mt-2"><a class="no-underline" href="/insight/tags/AI"><div class="text-sm bg-accent dark:bg-accent-dark text-white  px-2 py-1 rounded-md hover:bg-accent/70 dark:hover:bg-accent-dark/70">AI</div></a><a class="no-underline" href="/insight/tags/Generative%20AI"><div class="text-sm bg-accent dark:bg-accent-dark text-white  px-2 py-1 rounded-md hover:bg-accent/70 dark:hover:bg-accent-dark/70">Generative AI</div></a></div></div><div class="my-8"><div class="relative"><div class="absolute inset-0 bg-gray-700 bg-opacity-50 flex justify-center items-center rounded-lg p-4"><span class="text-white text-xl font-semibold text-center">Sorry, comment is under maintenance</span></div><div class="opacity-20 p-4"><input type="text" placeholder="Your email..." class="w-full p-2 mb-4 border-2 border-gray-400 rounded dark:bg-gray-700 placeholder-gray-500 dark:border-gray-700" disabled=""/><input type="text" placeholder="Write a comment..." class="w-full h-24 p-2 mb-4 border-2 border-gray-400 rounded dark:bg-gray-700 placeholder-gray-500 dark:border-gray-700" disabled=""/><button class="w-full text-white py-2 rounded transition-all bg-primary dark:bg-primary-dark hover:bg-primary/80 dark:hover:bg-primary/80" disabled="">Submit</button></div></div></div></article></div><aside class="lg:w-1/4 sticky max-h-[80vh] overflow-auto top-24 mb-24 self-start hidden lg:block"><div class="p-4 border-l-2 border-l-gray-200 dark:border-l-gray-800"><nav class="mb-8"><h2 class="ml-6 text-base font-semibold mb-2 text-gray-500">Contents</h2><ul class="list-none ml-6"><li class="mb-[0.3rem]
                      "><a href="#definition" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Definition</a></li><li class="mb-[0.3rem]
                      "><a href="#how-do-large-language-models-work" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">How do large language models work?</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#tokenization" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Tokenization</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#predicting-output-tokens" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Predicting output tokens</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#selection-process" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Selection process</a></li><li class="mb-[0.3rem]
                      "><a href="#custom-knowledge-base" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Custom Knowledge Base</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#embedding-knowledge-with-fine-tuning" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Embedding Knowledge with Fine-Tuning</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#use-knowledge-base-api-and-dynamic-retrieval" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Use Knowledge Base API and Dynamic Retrieval</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#contextual-knowledge-with-prompts" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Contextual Knowledge with Prompts</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#augmenting-with-vector-databases-eg-pinecone-faiss" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Augmenting with Vector Databases (e.g., Pinecone, FAISS)</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#hybrid-approach-with-langchain-and-fastapi" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Hybrid Approach with LangChain and FastAPI</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#document-retrieval-with-plugins-or-middleware" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Document Retrieval with Plugins or Middleware</a></li><li class="mb-[0.3rem]
                      ml-4"><a href="#hybrid-qa-pipelines-llm--traditional-search" class="text-xs text-gray-500 dark:text-gray-400 hover:text-secondary dark:hover:text-secondary 
                        
                        ">Hybrid QA Pipelines (LLM + Traditional Search)</a></li></ul></nav></div></aside></div></main><footer class="py-16 text-center bg-surface dark:bg-surface-dark w-full"><div class="container mx-auto p-4"><div class="flex flex-col md:flex-row md:items-end md:justify-between justify-center items-center space-y-1"><div><a href="https://irufano.com"><svg xmlns="http://www.w3.org/2000/svg" viewBox="654 387.275 995.842 187.45" width="995.842pt" height="187.45pt" fill="currentColor" class="h-6 w-auto md:h-6 text-text dark:text-white"><path d=" M 867.788 538.544 L 836.949 538.544 L 836.949 420.182 L 867.788 420.182 L 867.788 538.544 L 867.788 538.544 Z  M 924.471 538.544 L 893.632 538.544 L 893.632 420.182 L 962.375 420.182 L 962.375 420.182 Q 975.469 420.182 984.255 425.092 L 984.255 425.092 L 984.255 425.092 Q 993.042 430.003 997.435 438.531 L 997.435 438.531 L 997.435 438.531 Q 1001.829 447.059 1001.829 457.569 L 1001.829 457.569 L 1001.829 457.569 Q 1001.829 468.94 996.574 477.813 L 996.574 477.813 L 996.574 477.813 Q 991.319 486.686 981.843 491.682 L 981.843 491.682 L 1005.619 538.544 L 971.161 538.544 L 951.52 496.851 L 924.471 496.851 L 924.471 538.544 L 924.471 538.544 Z  M 924.471 443.613 L 924.471 473.936 L 956.345 473.936 L 956.345 473.936 Q 962.719 473.936 966.596 469.715 L 966.596 469.715 L 966.596 469.715 Q 970.472 465.494 970.472 458.43 L 970.472 458.43 L 970.472 458.43 Q 970.472 453.778 968.749 450.505 L 968.749 450.505 L 968.749 450.505 Q 967.026 447.231 963.925 445.422 L 963.925 445.422 L 963.925 445.422 Q 960.824 443.613 956.345 443.613 L 956.345 443.613 L 924.471 443.613 L 924.471 443.613 Z  M 1076.085 540.612 L 1076.085 540.612 L 1076.085 540.612 Q 1059.028 540.612 1046.71 535.271 L 1046.71 535.271 L 1046.71 535.271 Q 1034.391 529.93 1027.844 519.248 L 1027.844 519.248 L 1027.844 519.248 Q 1021.297 508.566 1021.297 492.716 L 1021.297 492.716 L 1021.297 420.182 L 1052.309 420.182 L 1052.309 492.026 L 1052.309 492.026 Q 1052.309 503.398 1058.339 509.944 L 1058.339 509.944 L 1058.339 509.944 Q 1064.369 516.491 1076.085 516.491 L 1076.085 516.491 L 1076.085 516.491 Q 1087.801 516.491 1094.003 509.944 L 1094.003 509.944 L 1094.003 509.944 Q 1100.205 503.398 1100.205 492.026 L 1100.205 492.026 L 1100.205 420.182 L 1131.045 420.182 L 1131.045 492.716 L 1131.045 492.716 Q 1131.045 508.566 1124.498 519.248 L 1124.498 519.248 L 1124.498 519.248 Q 1117.951 529.93 1105.719 535.271 L 1105.719 535.271 L 1105.719 535.271 Q 1093.486 540.612 1076.085 540.612 Z  M 1187.383 538.544 L 1156.544 538.544 L 1156.544 420.182 L 1247.34 420.182 L 1247.34 444.303 L 1187.383 444.303 L 1187.383 469.801 L 1240.104 469.801 L 1240.104 493.405 L 1187.383 493.405 L 1187.383 538.544 L 1187.383 538.544 Z  M 1277.663 538.544 L 1245.961 538.544 L 1290.412 420.182 L 1327.454 420.182 L 1371.904 538.544 L 1338.997 538.544 L 1331.933 518.042 L 1284.726 518.042 L 1277.663 538.544 L 1277.663 538.544 Z  M 1300.749 469.974 L 1292.135 494.955 L 1324.353 494.955 L 1315.911 469.974 L 1315.911 469.974 Q 1315.221 468.078 1314.274 465.236 L 1314.274 465.236 L 1314.274 465.236 Q 1313.326 462.393 1312.379 459.206 L 1312.379 459.206 L 1312.379 459.206 Q 1311.431 456.018 1310.57 452.745 L 1310.57 452.745 L 1310.57 452.745 Q 1309.708 449.471 1308.847 447.059 L 1308.847 447.059 L 1307.641 447.059 L 1307.641 447.059 Q 1306.952 450.333 1305.659 454.468 L 1305.659 454.468 L 1305.659 454.468 Q 1304.367 458.602 1303.075 462.737 L 1303.075 462.737 L 1303.075 462.737 Q 1301.783 466.872 1300.749 469.974 L 1300.749 469.974 L 1300.749 469.974 Z  M 1414.632 538.544 L 1386.032 538.544 L 1386.032 420.182 L 1413.426 420.182 L 1456.498 475.142 L 1456.498 475.142 Q 1457.704 476.52 1459.685 479.105 L 1459.685 479.105 L 1459.685 479.105 Q 1461.667 481.689 1463.562 484.36 L 1463.562 484.36 L 1463.562 484.36 Q 1465.457 487.03 1466.318 488.581 L 1466.318 488.581 L 1467.18 488.408 L 1467.18 488.408 Q 1467.18 484.618 1467.18 481.086 L 1467.18 481.086 L 1467.18 481.086 Q 1467.18 477.554 1467.18 475.142 L 1467.18 475.142 L 1467.18 420.182 L 1495.78 420.182 L 1495.78 538.544 L 1468.558 538.544 L 1422.902 480.655 L 1422.902 480.655 Q 1420.662 477.899 1418.681 474.97 L 1418.681 474.97 L 1418.681 474.97 Q 1416.699 472.041 1415.493 470.146 L 1415.493 470.146 L 1414.632 470.318 L 1414.632 470.318 Q 1414.632 473.075 1414.632 475.917 L 1414.632 475.917 L 1414.632 475.917 Q 1414.632 478.76 1414.632 480.655 L 1414.632 480.655 L 1414.632 538.544 L 1414.632 538.544 Z  M 1578.478 540.612 L 1578.478 540.612 L 1578.478 540.612 Q 1559.01 540.612 1545.14 533.806 L 1545.14 533.806 L 1545.14 533.806 Q 1531.271 527.001 1523.863 513.39 L 1523.863 513.39 L 1523.863 513.39 Q 1516.454 499.779 1516.454 479.277 L 1516.454 479.277 L 1516.454 479.277 Q 1516.454 458.775 1523.863 445.25 L 1523.863 445.25 L 1523.863 445.25 Q 1531.271 431.725 1545.14 424.92 L 1545.14 424.92 L 1545.14 424.92 Q 1559.01 418.115 1578.478 418.115 L 1578.478 418.115 L 1578.478 418.115 Q 1597.947 418.115 1611.816 424.92 L 1611.816 424.92 L 1611.816 424.92 Q 1625.685 431.725 1633.094 445.25 L 1633.094 445.25 L 1633.094 445.25 Q 1640.502 458.775 1640.502 479.277 L 1640.502 479.277 L 1640.502 479.277 Q 1640.502 499.779 1633.094 513.39 L 1633.094 513.39 L 1633.094 513.39 Q 1625.685 527.001 1611.816 533.806 L 1611.816 533.806 L 1611.816 533.806 Q 1597.947 540.612 1578.478 540.612 Z  M 1578.478 516.491 L 1578.478 516.491 L 1578.478 516.491 Q 1585.887 516.491 1591.572 514.252 L 1591.572 514.252 L 1591.572 514.252 Q 1597.258 512.012 1601.134 507.705 L 1601.134 507.705 L 1601.134 507.705 Q 1605.011 503.398 1606.906 497.195 L 1606.906 497.195 L 1606.906 497.195 Q 1608.801 490.993 1608.801 483.24 L 1608.801 483.24 L 1608.801 475.659 L 1608.801 475.659 Q 1608.801 467.734 1606.906 461.531 L 1606.906 461.531 L 1606.906 461.531 Q 1605.011 455.329 1601.134 451.022 L 1601.134 451.022 L 1601.134 451.022 Q 1597.258 446.715 1591.572 444.475 L 1591.572 444.475 L 1591.572 444.475 Q 1585.887 442.235 1578.478 442.235 L 1578.478 442.235 L 1578.478 442.235 Q 1570.898 442.235 1565.212 444.475 L 1565.212 444.475 L 1565.212 444.475 Q 1559.527 446.715 1555.736 451.022 L 1555.736 451.022 L 1555.736 451.022 Q 1551.946 455.329 1550.051 461.531 L 1550.051 461.531 L 1550.051 461.531 Q 1548.155 467.734 1548.155 475.659 L 1548.155 475.659 L 1548.155 483.24 L 1548.155 483.24 Q 1548.155 490.993 1550.051 497.195 L 1550.051 497.195 L 1550.051 497.195 Q 1551.946 503.398 1555.736 507.705 L 1555.736 507.705 L 1555.736 507.705 Q 1559.527 512.012 1565.212 514.252 L 1565.212 514.252 L 1565.212 514.252 Q 1570.898 516.491 1578.478 516.491 Z " fill="currentColor"></path><path d="M 696.047 532.572 L 660.118 496.643 C 652.037 488.562 651.95 475.528 659.924 467.554 L 659.924 467.554 C 667.898 459.58 680.932 459.667 689.013 467.748 L 724.942 503.677 C 733.023 511.758 733.11 524.792 725.136 532.766 L 725.136 532.766 C 717.162 540.74 704.128 540.653 696.047 532.572 Z" fill="#0CD79A"></path><path d="M 696.258 504.027 L 732.192 468.098 C 740.274 460.017 753.31 459.93 761.285 467.904 L 761.285 467.904 C 769.26 475.878 769.173 488.912 761.092 496.993 L 725.158 532.922 C 717.076 541.003 704.04 541.09 696.065 533.116 L 696.065 533.116 C 688.09 525.142 688.177 512.108 696.258 504.027 Z" fill="#46ECBA"></path><circle cx="710.6039726458765" cy="441.74053370304784" r="18.780480559001603" fill="rgb(12,215,154)"></circle></svg></a></div><img alt="-" loading="lazy" width="301" height="309" decoding="async" data-nimg="1" class="hidden md:inline-block w-ful w-4 h-auto" style="color:transparent" src="https://irufano.github.io/_next/static/media/irufano-square-logo.e74a10f7.svg"/><div class="flex items-center "><a href="/"><h3 class="text-sm text-gray-500">irufano.github.io<!-- --> <span class="font-light text-gray-400 dark:text-gray-600">|</span> <!-- -->2024</h3></a></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"gen-ai-concepts","meta":{"title":"Generative AI Key Concepts","date":"2024-09-24","description":"Machine learning is a subset of AI that involves developing algorithms that allow computers to learn from \u0026 make predictions or decisions based on data.","author":"irufano","tags":["AI","Generative AI"],"image":"https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/images/AI-diagram.png"},"content":"\u003ch2 id=\"definition\"\u003eDefinition\u003c/h2\u003e\n\u003cp\u003eGenerative AI refers to a type of artificial intelligence that creates new content, such as images, text, audio, or even video, based on patterns and data it has learned. What makes it a fantastic technology is that it democratizes AI, anyone can use it with as little as a text prompt, a sentence written in a natural language. There's no need for you to learn a language like Java or SQL to accomplish something worthwhile, all you need is to use your language, state what you want and out comes a suggestion from an AI model. The applications and impact for this is huge, you write or understand reports, write applications and much more, all in seconds.\u003c/p\u003e\n\u003cp\u003eAfter decades of research in the AI field, a new model architecture – called Transformer – overcame the limits of RNNs, being able to get much longer sequences of text as input. Transformers are based on the attention mechanism, enabling the model to give different weights to the inputs it receives, ‘paying more attention’ where the most relevant information is concentrated, regardless of their order in the text sequence.\u003c/p\u003e\n\u003cp\u003eMost of the recent generative AI models – also known as Large Language Models (LLMs), since they work with textual inputs and outputs – are indeed based on this architecture. What’s interesting about these models – trained on a huge amount of unlabeled data from diverse sources like books, articles and websites – is that they can be adapted to a wide variety of tasks and generate grammatically correct text with a semblance of creativity. So, not only did they incredibly enhance the capacity of a machine to ‘understand’ an input text, but they enabled their capacity to generate an original response in human language.\u003c/p\u003e\n\u003ch2 id=\"how-do-large-language-models-work\"\u003eHow do large language models work?\u003c/h2\u003e\n\u003ch3 id=\"tokenization\"\u003eTokenization\u003c/h3\u003e\n\u003cp\u003eLarge Language Models receive a text as input and generate a text as output. However, being statistical models, they work much better with numbers than text sequences. That’s why every input to the model is processed by a tokenizer, before being used by the core model. A token is a chunk of text – consisting of a variable number of characters, so the tokenizer's main task is splitting the input into an array of tokens. Then, each token is mapped with a token index, which is the integer encoding of the original text chunk.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/irufano/generative-ai-for-beginners/raw/main/01-introduction-to-genai/images/tokenizer-example.png?WT.mc_id=academic-105485-koreyst\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch3 id=\"predicting-output-tokens\"\u003ePredicting output tokens\u003c/h3\u003e\n\u003cp\u003eGiven n tokens as input (with max n varying from one model to another), the model is able to predict one token as output. This token is then incorporated into the input of the next iteration, in an expanding window pattern, enabling a better user experience of getting one (or multiple) sentence as an answer. This explains why, if you ever played with ChatGPT, you might have noticed that sometimes it looks like it stops in the middle of a sentence.\u003c/p\u003e\n\u003ch3 id=\"selection-process\"\u003eSelection process\u003c/h3\u003e\n\u003cp\u003eThe output token is chosen by the model according to its probability of occurring after the current text sequence. This is because the model predicts a probability distribution over all possible ‘next tokens’, calculated based on its training. However, not always the token with the highest probability is chosen from the resulting distribution. A degree of randomness is added to this choice, in a way that the model acts in a non-deterministic fashion - we do not get the exact same output for the same input. This degree of randomness is added to simulate the process of creative thinking and it can be tuned using a model parameter called temperature.\u003c/p\u003e\n\u003ch2 id=\"custom-knowledge-base\"\u003eCustom Knowledge Base\u003c/h2\u003e\n\u003cp\u003eTo add a knowledge base to an OpenAI-powered chatbot or application, you can use a variety of approaches depending on how dynamic and structured you want your knowledge base to be. Here are some common methods:\u003c/p\u003e\n\u003ch3 id=\"embedding-knowledge-with-fine-tuning\"\u003eEmbedding Knowledge with Fine-Tuning\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eFine-tuning\u003c/span\u003e: You can fine-tune the OpenAI model on your specific knowledge base or domain. This involves training the model on a custom dataset (such as product information, FAQs, internal documents) to improve its responses.\n\u003cul\u003e\n\u003cli\u003eSteps:\n\u003cul\u003e\n\u003cli\u003eCollect data (e.g., text documents, structured data).\u003c/li\u003e\n\u003cli\u003eFormat it into the proper input-output pairs (prompts and completions).\u003c/li\u003e\n\u003cli\u003eFine-tune the model using OpenAI's fine-tuning API.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eUse case\u003c/span\u003e: Best for cases where knowledge is mostly static and doesn’t change often.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"use-knowledge-base-api-and-dynamic-retrieval\"\u003eUse Knowledge Base API and Dynamic Retrieval\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eExternal APIs\u003c/span\u003e: Integrate the chatbot with a real-time knowledge base, such as a MySQL database, content management system, or external API.\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eMethod\u003c/span\u003e: Implement a retrieval mechanism that allows the chatbot to fetch answers from your dynamic database based on user queries. For instance, using vector embeddings to match user queries with the most relevant content in the database.\u003c/li\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eTools\u003c/span\u003e: You can use libraries like langchain to facilitate this process, creating a pipeline that pulls data dynamically based on user input.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eUse case\u003c/span\u003e: Useful when your knowledge base is large, changes frequently, or needs to handle complex queries.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contextual-knowledge-with-prompts\"\u003eContextual Knowledge with Prompts\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eLong prompts with context\u003c/span\u003e: You can inject specific information about your knowledge base directly into the prompt as context.\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eMethod\u003c/span\u003e: Before each query, construct a detailed prompt that includes relevant information from your knowledge base, which the model will use to provide context-aware answers.\u003c/li\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eUse case\u003c/span\u003e: Best for handling small sets of information or specific queries where the information needed is known beforehand.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"augmenting-with-vector-databases-eg-pinecone-faiss\"\u003eAugmenting with Vector Databases (e.g., Pinecone, FAISS)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eVector search\u003c/span\u003e: Store your knowledge base as embeddings in a vector database (like Pinecone, Weaviate, or FAISS). When a user asks a question, convert it to a vector, search for the most relevant knowledge entry, and use the result as part of the answer.\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eMethod\u003c/span\u003e: Convert text to embeddings using OpenAI or other models, and retrieve the most similar data points in response to a user query.\u003c/li\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eUse case:\u003c/span\u003e Perfect for large datasets where similarity search is required to find relevant answers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hybrid-approach-with-langchain-and-fastapi\"\u003eHybrid Approach with LangChain and FastAPI\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLangChain: LangChain enables building large language model (LLM) applications that can dynamically interact with databases and external systems. It helps set up a chatbot that can fetch data from knowledge bases like databases, APIs, and even web pages.\n\u003cul\u003e\n\u003cli\u003eSteps:\n\u003col\u003e\n\u003cli\u003eStore Knowledge: Define a knowledge base, which could be a SQL database, document store, or API.\u003c/li\u003e\n\u003cli\u003eQuery Execution: Use an LLM with the LangChain framework to query the knowledge base dynamically.\u003c/li\u003e\n\u003cli\u003eFastAPI Integration: Combine this with FastAPI for serving the chatbot with dynamic knowledge retrieval.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"document-retrieval-with-plugins-or-middleware\"\u003eDocument Retrieval with Plugins or Middleware\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOpenAI Plugins: If you're using OpenAI in an application, consider building or using a plugin or middleware that acts as a bridge to your knowledge base. For example, retrieving documents, querying databases, or even triggering workflows (e.g., customer support tickets) based on the query.\u003c/li\u003e\n\u003cli\u003eUse case: Ideal for more complex enterprise solutions, where OpenAI can function alongside existing systems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hybrid-qa-pipelines-llm--traditional-search\"\u003eHybrid QA Pipelines (LLM + Traditional Search)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eCombining LLMs with traditional search\u003c/span\u003e: You can combine the capabilities of language models with traditional search mechanisms like Elasticsearch or other keyword-based systems.\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eMethod\u003c/span\u003e: First, use search engines to retrieve relevant documents, and then pass those documents to OpenAI for processing and summarization.\u003c/li\u003e\n\u003cli\u003e\u003cspan class=\"strong-block\"\u003eUse case\u003c/span\u003e: Works well in applications where quick retrieval from a large corpus of documents is needed, and then those documents are summarized or clarified by the LLM.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","sections":[{"level":"2","id":"definition","text":"Definition"},{"level":"2","id":"how-do-large-language-models-work","text":"How do large language models work?"},{"level":"3","id":"tokenization","text":"Tokenization"},{"level":"3","id":"predicting-output-tokens","text":"Predicting output tokens"},{"level":"3","id":"selection-process","text":"Selection process"},{"level":"2","id":"custom-knowledge-base","text":"Custom Knowledge Base"},{"level":"3","id":"embedding-knowledge-with-fine-tuning","text":"Embedding Knowledge with Fine-Tuning"},{"level":"3","id":"use-knowledge-base-api-and-dynamic-retrieval","text":"Use Knowledge Base API and Dynamic Retrieval"},{"level":"3","id":"contextual-knowledge-with-prompts","text":"Contextual Knowledge with Prompts"},{"level":"3","id":"augmenting-with-vector-databases-eg-pinecone-faiss","text":"Augmenting with Vector Databases (e.g., Pinecone, FAISS)"},{"level":"3","id":"hybrid-approach-with-langchain-and-fastapi","text":"Hybrid Approach with LangChain and FastAPI"},{"level":"3","id":"document-retrieval-with-plugins-or-middleware","text":"Document Retrieval with Plugins or Middleware"},{"level":"3","id":"hybrid-qa-pipelines-llm--traditional-search","text":"Hybrid QA Pipelines (LLM + Traditional Search)"}],"raw":"\n## Definition\n\nGenerative AI refers to a type of artificial intelligence that creates new content, such as images, text, audio, or even video, based on patterns and data it has learned. What makes it a fantastic technology is that it democratizes AI, anyone can use it with as little as a text prompt, a sentence written in a natural language. There's no need for you to learn a language like Java or SQL to accomplish something worthwhile, all you need is to use your language, state what you want and out comes a suggestion from an AI model. The applications and impact for this is huge, you write or understand reports, write applications and much more, all in seconds.\n\nAfter decades of research in the AI field, a new model architecture – called Transformer – overcame the limits of RNNs, being able to get much longer sequences of text as input. Transformers are based on the attention mechanism, enabling the model to give different weights to the inputs it receives, ‘paying more attention’ where the most relevant information is concentrated, regardless of their order in the text sequence.\n\nMost of the recent generative AI models – also known as Large Language Models (LLMs), since they work with textual inputs and outputs – are indeed based on this architecture. What’s interesting about these models – trained on a huge amount of unlabeled data from diverse sources like books, articles and websites – is that they can be adapted to a wide variety of tasks and generate grammatically correct text with a semblance of creativity. So, not only did they incredibly enhance the capacity of a machine to ‘understand’ an input text, but they enabled their capacity to generate an original response in human language.\n\n## How do large language models work?\n\n### Tokenization\n\nLarge Language Models receive a text as input and generate a text as output. However, being statistical models, they work much better with numbers than text sequences. That’s why every input to the model is processed by a tokenizer, before being used by the core model. A token is a chunk of text – consisting of a variable number of characters, so the tokenizer's main task is splitting the input into an array of tokens. Then, each token is mapped with a token index, which is the integer encoding of the original text chunk.\n\n![image](https://github.com/irufano/generative-ai-for-beginners/raw/main/01-introduction-to-genai/images/tokenizer-example.png?WT.mc_id=academic-105485-koreyst)\n\n### Predicting output tokens\n\nGiven n tokens as input (with max n varying from one model to another), the model is able to predict one token as output. This token is then incorporated into the input of the next iteration, in an expanding window pattern, enabling a better user experience of getting one (or multiple) sentence as an answer. This explains why, if you ever played with ChatGPT, you might have noticed that sometimes it looks like it stops in the middle of a sentence.\n\n### Selection process\n\nThe output token is chosen by the model according to its probability of occurring after the current text sequence. This is because the model predicts a probability distribution over all possible ‘next tokens’, calculated based on its training. However, not always the token with the highest probability is chosen from the resulting distribution. A degree of randomness is added to this choice, in a way that the model acts in a non-deterministic fashion - we do not get the exact same output for the same input. This degree of randomness is added to simulate the process of creative thinking and it can be tuned using a model parameter called temperature.\n\n## Custom Knowledge Base\n\nTo add a knowledge base to an OpenAI-powered chatbot or application, you can use a variety of approaches depending on how dynamic and structured you want your knowledge base to be. Here are some common methods:\n\n###  Embedding Knowledge with Fine-Tuning\n- **Fine-tuning**: You can fine-tune the OpenAI model on your specific knowledge base or domain. This involves training the model on a custom dataset (such as product information, FAQs, internal documents) to improve its responses.\n  - Steps:\n    - Collect data (e.g., text documents, structured data).\n    - Format it into the proper input-output pairs (prompts and completions).\n    - Fine-tune the model using OpenAI's fine-tuning API.\n- **Use case**: Best for cases where knowledge is mostly static and doesn’t change often.\n\n### Use Knowledge Base API and Dynamic Retrieval\n\n- **External APIs**: Integrate the chatbot with a real-time knowledge base, such as a MySQL database, content management system, or external API.\n  - **Method**: Implement a retrieval mechanism that allows the chatbot to fetch answers from your dynamic database based on user queries. For instance, using vector embeddings to match user queries with the most relevant content in the database.\n  - **Tools**: You can use libraries like langchain to facilitate this process, creating a pipeline that pulls data dynamically based on user input.\n- **Use case**: Useful when your knowledge base is large, changes frequently, or needs to handle complex queries.\n\n### Contextual Knowledge with Prompts\n\n- **Long prompts with context**: You can inject specific information about your knowledge base directly into the prompt as context.\n  - **Method**: Before each query, construct a detailed prompt that includes relevant information from your knowledge base, which the model will use to provide context-aware answers.\n  - **Use case**: Best for handling small sets of information or specific queries where the information needed is known beforehand.\n\n### Augmenting with Vector Databases (e.g., Pinecone, FAISS)\n\n- **Vector search**: Store your knowledge base as embeddings in a vector database (like Pinecone, Weaviate, or FAISS). When a user asks a question, convert it to a vector, search for the most relevant knowledge entry, and use the result as part of the answer.\n  - **Method**: Convert text to embeddings using OpenAI or other models, and retrieve the most similar data points in response to a user query.\n  - **Use case:** Perfect for large datasets where similarity search is required to find relevant answers.\n\n### Hybrid Approach with LangChain and FastAPI\n\n- LangChain: LangChain enables building large language model (LLM) applications that can dynamically interact with databases and external systems. It helps set up a chatbot that can fetch data from knowledge bases like databases, APIs, and even web pages.\n  - Steps:\n    1. Store Knowledge: Define a knowledge base, which could be a SQL database, document store, or API.\n    2. Query Execution: Use an LLM with the LangChain framework to query the knowledge base dynamically.\n    3. FastAPI Integration: Combine this with FastAPI for serving the chatbot with dynamic knowledge retrieval.\n\n### Document Retrieval with Plugins or Middleware\n\n- OpenAI Plugins: If you're using OpenAI in an application, consider building or using a plugin or middleware that acts as a bridge to your knowledge base. For example, retrieving documents, querying databases, or even triggering workflows (e.g., customer support tickets) based on the query.\n- Use case: Ideal for more complex enterprise solutions, where OpenAI can function alongside existing systems.\n\n### Hybrid QA Pipelines (LLM + Traditional Search)\n\n- **Combining LLMs with traditional search**: You can combine the capabilities of language models with traditional search mechanisms like Elasticsearch or other keyword-based systems.\n  - **Method**: First, use search engines to retrieve relevant documents, and then pass those documents to OpenAI for processing and summarization.\n  - **Use case**: Works well in applications where quick retrieval from a large corpus of documents is needed, and then those documents are summarized or clarified by the LLM.","readingTime":"7 min read"}},"__N_SSG":true},"page":"/insight/post/[slug]","query":{"slug":"gen-ai-concepts"},"buildId":"DLEiHtd6RpNZKc-Yv-HAa","assetPrefix":"https://irufano.github.io","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>